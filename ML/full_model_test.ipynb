{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0fb187ceb99e597fc4fb087033f2a56d4997f1306a985d00c1ac5ca0a4e888195",
   "display_name": "Python 3.7.9 64-bit ('mlenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "source": [
    "## Import data from s3.amazonaws"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_url = 'https://s3.amazonaws.com/parkerhiggins-nba-draft-bucket/MBB_StatsAndDraft.csv'\n",
    "\n",
    "raw_df = pd.read_csv(cleaned_data_url)"
   ]
  },
  {
   "source": [
    "## Preprocessing: Remove non-numerical columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = raw_df.dtypes[raw_df.dtypes != 'object'].index.tolist()\n",
    "\n",
    "numerical_df = raw_df[numerical_cols]\n",
    "numerical_df.index = raw_df['#']"
   ]
  },
  {
   "source": [
    "## Preprocessing: Select common individual basketball statistics to use as features in the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['PPG','MPG','RPG','APG','SPG','GP','TOV','Pk']\n",
    "data_df = numerical_df[selected_features]\n",
    "\n",
    "X = data_df.drop(columns=['Pk'])\n",
    "y = data_df['Pk'].apply(lambda x: 1 if x <= 60 else 2)"
   ]
  },
  {
   "source": [
    "## Preprocessing: Use SMOTEENN to oversample drafted players and undersample undrafted players"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoteenn = SMOTEENN(random_state=1)\n",
    "X_resampled, y_resampled = smoteenn.fit_resample(X,y)"
   ]
  },
  {
   "source": [
    "## Preprocessing: Split into training and test datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,random_state=1)"
   ]
  },
  {
   "source": [
    "## Preprocessing: Scale data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)"
   ]
  },
  {
   "source": [
    "# Random Forest Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9708654670094259 \n\n[[6403   92]\n [ 248 4927]] \n\n              precision    recall  f1-score   support\n\n           1       0.96      0.99      0.97      6495\n           2       0.98      0.95      0.97      5175\n\n    accuracy                           0.97     11670\n   macro avg       0.97      0.97      0.97     11670\nweighted avg       0.97      0.97      0.97     11670\n \n\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=120,random_state=1)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "rf_acc_score = accuracy_score(y_test,rf_pred)\n",
    "rf_matrix = confusion_matrix(y_test,rf_pred)\n",
    "rf_results = pd.DataFrame({\"Prediction\": rf_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "rf_report = classification_report(y_test,rf_pred,zero_division=True)\n",
    "\n",
    "print(rf_acc_score,'\\n')\n",
    "print(rf_matrix,'\\n')\n",
    "print(rf_report,'\\n')"
   ]
  },
  {
   "source": [
    "# Logistic Regression Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7974293059125964 \n\n[[5341 1154]\n [1210 3965]] \n\n              precision    recall  f1-score   support\n\n           1       0.82      0.82      0.82      6495\n           2       0.77      0.77      0.77      5175\n\n    accuracy                           0.80     11670\n   macro avg       0.79      0.79      0.79     11670\nweighted avg       0.80      0.80      0.80     11670\n \n\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs',random_state=1, max_iter=1000)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "lr_pred = classifier.predict(X_test)\n",
    "\n",
    "lr_acc_score = accuracy_score(y_test,lr_pred)\n",
    "lr_matrix = confusion_matrix(y_test,lr_pred)\n",
    "lr_results = pd.DataFrame({\"Prediction\": lr_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "lr_report = classification_report(y_test,lr_pred,zero_division=True)\n",
    "\n",
    "print(lr_acc_score,'\\n')\n",
    "print(lr_matrix,'\\n')\n",
    "print(lr_report,'\\n')"
   ]
  },
  {
   "source": [
    "# Support Vector Machines (SVM) Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7949443016281063 \n\n[[5286 1209]\n [1184 3991]] \n\n              precision    recall  f1-score   support\n\n           1       0.82      0.81      0.82      6495\n           2       0.77      0.77      0.77      5175\n\n    accuracy                           0.79     11670\n   macro avg       0.79      0.79      0.79     11670\nweighted avg       0.80      0.79      0.79     11670\n \n\n"
     ]
    }
   ],
   "source": [
    "svm_model = svm.SVC(kernel='linear')\n",
    "svm_model.fit(X_train,y_train)\n",
    "\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "svm_acc_score = accuracy_score(y_test,svm_pred)\n",
    "svm_matrix = confusion_matrix(y_test,svm_pred)\n",
    "svm_results = pd.DataFrame({\"Prediction\": svm_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "svm_report = classification_report(y_test,svm_pred,zero_division=True)\n",
    "\n",
    "print(svm_acc_score,'\\n')\n",
    "print(svm_matrix,'\\n')\n",
    "print(svm_report,'\\n')"
   ]
  },
  {
   "source": [
    "# Gradient Boosting Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "learning_rate=0.05; training: 0.809809466678093\n",
      "learning_rate=0.05; validation: 0.8059982862039418\n",
      "learning_rate=0.1; training: 0.8160939240723284\n",
      "learning_rate=0.1; validation: 0.8139674378748929\n",
      "learning_rate=0.25; training: 0.8419744622504071\n",
      "learning_rate=0.25; validation: 0.8424164524421593\n",
      "learning_rate=0.5; training: 0.913445882252121\n",
      "learning_rate=0.5; validation: 0.9107969151670952\n",
      "learning_rate=0.75; training: 0.936098494586797\n",
      "learning_rate=0.75; validation: 0.9298200514138818\n",
      "learning_rate=1; training: 0.9385551461136344\n",
      "learning_rate=1; validation: 0.9352185089974293\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.05,0.1,0.25,0.5,0.75,1]\n",
    "\n",
    "for rate in learning_rates:\n",
    "    classifier = GradientBoostingClassifier(n_estimators=20,learning_rate=rate,random_state=1)\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    print(f'learning_rate={rate}; training: {classifier.score(X_train,y_train)}')\n",
    "    print(f'learning_rate={rate}; validation: {classifier.score(X_test,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9352185089974293 \n\n[[6068  427]\n [ 329 4846]] \n\n              precision    recall  f1-score   support\n\n           1       0.95      0.93      0.94      6495\n           2       0.92      0.94      0.93      5175\n\n    accuracy                           0.94     11670\n   macro avg       0.93      0.94      0.93     11670\nweighted avg       0.94      0.94      0.94     11670\n \n\n"
     ]
    }
   ],
   "source": [
    "classifier = GradientBoostingClassifier(n_estimators=20,learning_rate=1,random_state=1)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "gb_pred = classifier.predict(X_test)\n",
    "\n",
    "gb_acc_score = accuracy_score(y_test,gb_pred)\n",
    "gb_matrix = confusion_matrix(y_test,gb_pred)\n",
    "gb_results = pd.DataFrame({\"Prediction\": gb_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "gb_report = classification_report(y_test,gb_pred,zero_division=True)\n",
    "\n",
    "print(gb_acc_score,'\\n')\n",
    "print(gb_matrix,'\\n')\n",
    "print(gb_report,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0fb187ceb99e597fc4fb087033f2a56d4997f1306a985d00c1ac5ca0a4e888195",
   "display_name": "Python 3.7.9 64-bit ('mlenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Import data from s3.amazonaws"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_url = 'https://s3.amazonaws.com/parkerhiggins-nba-draft-bucket/MBB_StatsAndDraft.csv'\n",
    "data_2007_2020 = 'https://s3.amazonaws.com/parkerhiggins-nba-draft-bucket/07-20_MBB_StatsAndDraft.csv'\n",
    "data_2007_2019 = 'https://s3.amazonaws.com/parkerhiggins-nba-draft-bucket/07-19_MBB_StatsAndDraft.csv'\n",
    "\n",
    "raw_df = pd.read_csv(data_2007_2019)"
   ]
  },
  {
   "source": [
    "## Preprocessing: Remove non-numerical columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = raw_df.dtypes[raw_df.dtypes != 'object'].index.tolist()\n",
    "\n",
    "numerical_df = raw_df[numerical_cols]\n",
    "numerical_df.index = raw_df['#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       #  gp   mpg  fgm  fga    FG%  3PM  3PA   3P%  ftm  ...  drb   rpg  apg  \\\n",
       "#                                                         ...                   \n",
       "240  240  33  28.4  5.7  9.9  0.573  0.0  0.1  0.00  3.4  ...  5.9   9.2  1.8   \n",
       "250  250  40  32.0  5.2  8.4  0.623  0.1  0.5  0.15  3.6  ...  7.4  10.4  1.2   \n",
       "\n",
       "     spg  bpg   ppg  season_year   pk  draft_year  draft_status  \n",
       "#                                                                \n",
       "240  1.0  0.8  14.8         2008  1.0      2009.0         False  \n",
       "250  1.4  4.7  14.2         2012  1.0      2012.0          True  \n",
       "\n",
       "[2 rows x 25 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#</th>\n      <th>gp</th>\n      <th>mpg</th>\n      <th>fgm</th>\n      <th>fga</th>\n      <th>FG%</th>\n      <th>3PM</th>\n      <th>3PA</th>\n      <th>3P%</th>\n      <th>ftm</th>\n      <th>...</th>\n      <th>drb</th>\n      <th>rpg</th>\n      <th>apg</th>\n      <th>spg</th>\n      <th>bpg</th>\n      <th>ppg</th>\n      <th>season_year</th>\n      <th>pk</th>\n      <th>draft_year</th>\n      <th>draft_status</th>\n    </tr>\n    <tr>\n      <th>#</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>240</th>\n      <td>240</td>\n      <td>33</td>\n      <td>28.4</td>\n      <td>5.7</td>\n      <td>9.9</td>\n      <td>0.573</td>\n      <td>0.0</td>\n      <td>0.1</td>\n      <td>0.00</td>\n      <td>3.4</td>\n      <td>...</td>\n      <td>5.9</td>\n      <td>9.2</td>\n      <td>1.8</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>14.8</td>\n      <td>2008</td>\n      <td>1.0</td>\n      <td>2009.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>250</th>\n      <td>250</td>\n      <td>40</td>\n      <td>32.0</td>\n      <td>5.2</td>\n      <td>8.4</td>\n      <td>0.623</td>\n      <td>0.1</td>\n      <td>0.5</td>\n      <td>0.15</td>\n      <td>3.6</td>\n      <td>...</td>\n      <td>7.4</td>\n      <td>10.4</td>\n      <td>1.2</td>\n      <td>1.4</td>\n      <td>4.7</td>\n      <td>14.2</td>\n      <td>2012</td>\n      <td>1.0</td>\n      <td>2012.0</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 25 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "numerical_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "numerical_df['draft_status'] = (numerical_df['season_year']==numerical_df['draft_year']).astype(bool)"
   ]
  },
  {
   "source": [
    "## Preprocessing: Select common individual basketball statistics to use as features in the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['ppg','mpg','rpg','apg','spg','gp','tov','draft_status']\n",
    "data_df = numerical_df[selected_features]\n",
    "\n",
    "\n",
    "X = data_df.drop(columns=['draft_status'])\n",
    "y = data_df['draft_status']"
   ]
  },
  {
   "source": [
    "## Preprocessing: Use SMOTEENN to oversample drafted players and undersample undrafted players"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoteenn = SMOTEENN(random_state=1)\n",
    "X_resampled, y_resampled = smoteenn.fit_resample(X,y)"
   ]
  },
  {
   "source": [
    "## Preprocessing: Split into training and test datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,random_state=1)"
   ]
  },
  {
   "source": [
    "## Preprocessing: Scale data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)"
   ]
  },
  {
   "source": [
    "# Random Forest Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9859403940057249 \n\n[[5534  127]\n [  40 6177]] \n\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.99      5661\n        True       0.98      0.99      0.99      6217\n\n    accuracy                           0.99     11878\n   macro avg       0.99      0.99      0.99     11878\nweighted avg       0.99      0.99      0.99     11878\n \n\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=120,random_state=1)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "rf_acc_score = accuracy_score(y_test,rf_pred)\n",
    "rf_matrix = confusion_matrix(y_test,rf_pred)\n",
    "rf_results = pd.DataFrame({\"Prediction\": rf_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "rf_report = classification_report(y_test,rf_pred,zero_division=True)\n",
    "\n",
    "print(rf_acc_score,'\\n')\n",
    "print(rf_matrix,'\\n')\n",
    "print(rf_report,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'rf_2007_2019.sav'\n",
    "pickle.dump(rf, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Code below this cell is not used in the 2007 - 2019 Random Forest Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Logistic Regression Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8026440037771483 \n\n[[5372 1163]\n [1136 3978]] \n\n              precision    recall  f1-score   support\n\n           1       0.83      0.82      0.82      6535\n           2       0.77      0.78      0.78      5114\n\n    accuracy                           0.80     11649\n   macro avg       0.80      0.80      0.80     11649\nweighted avg       0.80      0.80      0.80     11649\n \n\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs',random_state=1, max_iter=1000)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "lr_pred = classifier.predict(X_test)\n",
    "\n",
    "lr_acc_score = accuracy_score(y_test,lr_pred)\n",
    "lr_matrix = confusion_matrix(y_test,lr_pred)\n",
    "lr_results = pd.DataFrame({\"Prediction\": lr_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "lr_report = classification_report(y_test,lr_pred,zero_division=True)\n",
    "\n",
    "print(lr_acc_score,'\\n')\n",
    "print(lr_matrix,'\\n')\n",
    "print(lr_report,'\\n')"
   ]
  },
  {
   "source": [
    "# Support Vector Machines (SVM) Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8026440037771483 \n\n[[5331 1204]\n [1095 4019]] \n\n              precision    recall  f1-score   support\n\n           1       0.83      0.82      0.82      6535\n           2       0.77      0.79      0.78      5114\n\n    accuracy                           0.80     11649\n   macro avg       0.80      0.80      0.80     11649\nweighted avg       0.80      0.80      0.80     11649\n \n\n"
     ]
    }
   ],
   "source": [
    "svm_model = svm.SVC(kernel='linear')\n",
    "svm_model.fit(X_train,y_train)\n",
    "\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "svm_acc_score = accuracy_score(y_test,svm_pred)\n",
    "svm_matrix = confusion_matrix(y_test,svm_pred)\n",
    "svm_results = pd.DataFrame({\"Prediction\": svm_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "svm_report = classification_report(y_test,svm_pred,zero_division=True)\n",
    "\n",
    "print(svm_acc_score,'\\n')\n",
    "print(svm_matrix,'\\n')\n",
    "print(svm_report,'\\n')"
   ]
  },
  {
   "source": [
    "# Gradient Boosting Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "learning_rate=0.05; training: 0.8057343978023864\n",
      "learning_rate=0.05; validation: 0.8091681689415401\n",
      "learning_rate=0.1; training: 0.8150341946375941\n",
      "learning_rate=0.1; validation: 0.8174092196755086\n",
      "learning_rate=0.25; training: 0.85034480785189\n",
      "learning_rate=0.25; validation: 0.8483990042063696\n",
      "learning_rate=0.5; training: 0.8874581509142415\n",
      "learning_rate=0.5; validation: 0.881878272813117\n",
      "learning_rate=0.75; training: 0.9330128480270123\n",
      "learning_rate=0.75; validation: 0.924113657824706\n",
      "learning_rate=1; training: 0.9415686611154033\n",
      "learning_rate=1; validation: 0.9341574384067302\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.05,0.1,0.25,0.5,0.75,1]\n",
    "\n",
    "for rate in learning_rates:\n",
    "    classifier = GradientBoostingClassifier(n_estimators=20,learning_rate=rate,random_state=1)\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    print(f'learning_rate={rate}; training: {classifier.score(X_train,y_train)}')\n",
    "    print(f'learning_rate={rate}; validation: {classifier.score(X_test,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9341574384067302 \n\n[[6129  406]\n [ 361 4753]] \n\n              precision    recall  f1-score   support\n\n           1       0.94      0.94      0.94      6535\n           2       0.92      0.93      0.93      5114\n\n    accuracy                           0.93     11649\n   macro avg       0.93      0.93      0.93     11649\nweighted avg       0.93      0.93      0.93     11649\n \n\n"
     ]
    }
   ],
   "source": [
    "classifier = GradientBoostingClassifier(n_estimators=20,learning_rate=1,random_state=1)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "gb_pred = classifier.predict(X_test)\n",
    "\n",
    "gb_acc_score = accuracy_score(y_test,gb_pred)\n",
    "gb_matrix = confusion_matrix(y_test,gb_pred)\n",
    "gb_results = pd.DataFrame({\"Prediction\": gb_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "gb_report = classification_report(y_test,gb_pred,zero_division=True)\n",
    "\n",
    "print(gb_acc_score,'\\n')\n",
    "print(gb_matrix,'\\n')\n",
    "print(gb_report,'\\n')"
   ]
  },
  {
   "source": [
    "# Save and export the random forest model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'nba_rf_model.sav'\n",
    "pickle.dump(rf, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9714138552665464\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(model_filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}